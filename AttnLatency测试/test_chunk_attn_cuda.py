import unittest
import torch
from chunk_attn import Attention
import random
import math
import gc
import time
import matplotlib.pyplot as plt



class TestChunkAttnGPU(unittest.TestCase):
    torch.cuda.empty_cache()
    def setUp(self) -> None:
        self.device = torch.device("cuda:1")  # æŒ‡å®šä½¿ç”¨cuda:1
        self.dtype = torch.float16
        torch.set_default_dtype(self.dtype)
        # è®¾ç½®é»˜è®¤è®¾å¤‡ä¸ºcuda:1
        torch.cuda.set_device(self.device)
        print(f"device:{self.device} dtype:{self.dtype}")

    def tearDown(self) -> None:
        pass

    

    def test_different_configs(self):
        # é…ç½®å‚æ•°
        torch.cuda.empty_cache()
        seq_lengths = [1024,2048, 4096, 6144, 8192, 10240, 12288, 14336, 16384, 18432, 20480, 22528, 24576, 26624, 28672, 30720, 32768, 34816, 36864, 38912, 40960, 43008, 45056, 47104, 49152, 51200, 53248, 55296, 57344, 59392, 61440, 63488, 65536, 67584, 69632, 71680, 73728, 75776, 77824, 79872, 81920, 83968, 86016, 88064, 90112, 92160, 94208, 96256, 98304, 100352, 102400, 104448, 106496, 108544, 110592, 112640, 114688, 116736, 118784, 120832, 122880, 124928, 126976, 129024, 131072, 133120, 135168, 137216, 139264, 141312, 143360, 145408, 147456, 149504, 151552, 153600, 155648, 157696, 159744, 161792, 163840, 165888, 167936, 169984, 172032, 174080, 176128, 178176, 180224, 182272, 184320, 186368, 188416, 190464, 192512, 194560, 196608, 198656, 200704, 202752, 204800, 206848, 208896, 210944, 212992, 215040, 217088, 219136, 221184, 223232, 225280, 227328, 229376, 231424, 233472, 235520, 237568, 239616, 241664, 243712, 245760, 247808, 249856, 251904, 253952, 256000, 258048, 260096, 262144, 264192, 266240, 268288, 270336, 272384, 274432, 276480, 278528, 280576, 282624, 284672, 286720, 288768, 290816, 292864, 294912, 296960, 299008, 301056, 303104, 305152, 307200, 309248, 311296, 313344, 315392, 317440, 319488, 321536, 323584, 325632, 327680, 329728, 331776, 333824, 335872, 337920, 339968, 342016, 344064, 346112, 348160, 350208, 352256, 354304, 356352, 358400, 360448, 362496, 364544, 366592, 368640, 370688, 372736, 374784, 376832, 378880, 380928, 382976, 385024, 387072, 389120, 391168, 393216, 395264, 397312, 399360, 401408, 403456, 405504, 407552, 409600, 411648, 413696, 415744, 417792, 419840, 421888, 423936, 425984, 428032, 430080, 432128, 434176, 436224, 438272, 440320, 442368, 444416, 446464, 448512, 450560, 452608, 454656, 456704, 458752, 460800, 462848, 464896, 466944, 468992, 471040, 473088, 475136, 477184, 479232, 481280, 483328, 485376, 487424, 489472, 491520, 493568, 495616, 497664, 499712, 501760, 503808, 505856, 507904, 509952, 512000, 514048, 516096, 518144, 520192, 522240, 524288, 526336, 528384, 530432, 532480, 534528, 536576, 538624, 540672, 542720, 544768, 546816, 548864, 550912, 552960, 555008, 557056, 559104, 561152, 563200, 565248, 567296, 569344, 571392, 573440, 575488, 577536, 579584, 581632, 583680, 585728, 587776, 589824, 591872, 593920, 595968, 598016, 600064, 602112, 604160, 606208, 608256, 610304, 612352, 614400, 616448, 618496, 620544, 622592, 624640, 626688, 628736, 630784, 632832, 634880, 636928, 638976, 641024, 643072, 645120, 647168, 649216, 651264, 653312, 655360, 657408, 659456, 661504, 663552, 665600, 667648, 669696, 671744, 673792, 675840, 677888, 679936, 681984, 684032, 686080, 688128, 690176, 692224, 694272, 696320, 698368, 700416, 702464, 704512, 706560, 708608, 710656, 712704, 714752, 716800, 718848, 720896, 722944, 724992, 727040, 729088, 731136, 733184, 735232, 737280, 739328, 741376, 743424, 745472, 747520, 749568, 751616, 753664, 755712, 757760, 759808, 761856, 763904, 765952, 768000, 770048, 772096, 774144, 776192, 778240, 780288, 782336, 784384, 786432, 788480, 790528, 792576, 794624, 796672, 798720, 800768, 802816, 804864, 806912, 808960, 811008, 813056, 815104, 817152, 819200, 821248, 823296, 825344, 827392, 829440, 831488, 833536, 835584, 837632, 839680, 841728, 843776, 845824, 847872, 849920, 851968, 854016, 856064, 858112, 860160, 862208, 864256, 866304, 868352, 870400, 872448, 874496, 876544, 878592, 880640, 882688, 884736, 886784, 888832, 890880, 892928, 894976, 897024, 899072, 901120, 903168, 905216, 907264, 909312, 911360, 913408, 915456, 917504, 919552, 921600, 923648, 925696, 927744, 929792, 931840, 933888, 935936, 937984, 940032, 942080, 944128, 946176, 948224, 950272, 952320, 954368, 956416, 958464, 960512, 962560, 964608, 966656, 968704, 970752, 972800, 974848, 976896, 978944, 980992, 983040, 985088, 987136, 989184, 991232, 993280, 995328, 997376, 999424, 1001472, 1003520, 1005568, 1007616, 1009664, 1011712, 1013760, 1015808, 1017856, 1019904, 1021952, 1024000, 1026048, 1028096, 1030144, 1032192, 1034240, 1036288, 1038336, 1040384, 1042432, 1044480, 1046528, 1048576]
        #[1024,2048, 4096, 6144, 8192, 10240, 12288, 14336, 16384, 18432, 20480, 22528, 24576, 26624, 28672, 30720, 32768, 34816, 36864, 38912, 40960, 43008, 45056, 47104, 49152, 51200, 53248, 55296, 57344, 59392, 61440, 63488, 65536, 67584, 69632, 71680, 73728, 75776, 77824, 79872, 81920, 83968, 86016, 88064, 90112, 92160, 94208, 96256, 98304, 100352, 102400, 104448, 106496, 108544, 110592, 112640, 114688, 116736, 118784, 120832, 122880, 124928, 126976, 129024, 131072, 133120, 135168, 137216, 139264, 141312, 143360, 145408, 147456, 149504, 151552, 153600, 155648, 157696, 159744, 161792, 163840, 165888, 167936, 169984, 172032, 174080, 176128, 178176, 180224, 182272, 184320, 186368, 188416, 190464, 192512, 194560, 196608, 198656, 200704, 202752, 204800, 206848, 208896, 210944, 212992, 215040, 217088, 219136, 221184, 223232, 225280, 227328, 229376, 231424, 233472, 235520, 237568, 239616, 241664, 243712, 245760, 247808, 249856, 251904, 253952, 256000, 258048, 260096, 262144, 264192, 266240, 268288, 270336, 272384, 274432, 276480, 278528, 280576, 282624, 284672, 286720, 288768, 290816, 292864, 294912, 296960, 299008, 301056, 303104, 305152, 307200, 309248, 311296, 313344, 315392, 317440, 319488, 321536, 323584, 325632, 327680, 329728, 331776, 333824, 335872, 337920, 339968, 342016, 344064, 346112, 348160, 350208, 352256, 354304, 356352, 358400, 360448, 362496, 364544, 366592, 368640, 370688, 372736, 374784, 376832, 378880, 380928, 382976, 385024, 387072, 389120, 391168, 393216, 395264, 397312, 399360, 401408, 403456, 405504, 407552, 409600, 411648, 413696, 415744, 417792, 419840, 421888, 423936, 425984, 428032, 430080, 432128, 434176, 436224, 438272, 440320, 442368, 444416, 446464, 448512, 450560, 452608, 454656, 456704, 458752, 460800, 462848, 464896, 466944, 468992, 471040, 473088, 475136, 477184, 479232, 481280, 483328, 485376, 487424, 489472, 491520, 493568, 495616, 497664, 499712, 501760, 503808, 505856, 507904, 509952, 512000, 514048, 516096, 518144, 520192, 522240, 524288, 526336, 528384, 530432, 532480, 534528, 536576, 538624, 540672, 542720, 544768, 546816, 548864, 550912, 552960, 555008, 557056, 559104, 561152, 563200, 565248, 567296, 569344, 571392, 573440, 575488, 577536, 579584, 581632, 583680, 585728, 587776, 589824, 591872, 593920, 595968, 598016, 600064, 602112, 604160, 606208, 608256, 610304, 612352, 614400, 616448, 618496, 620544, 622592, 624640, 626688, 628736, 630784, 632832, 634880, 636928, 638976, 641024, 643072, 645120, 647168, 649216, 651264, 653312, 655360, 657408, 659456, 661504, 663552, 665600, 667648, 669696, 671744, 673792, 675840, 677888, 679936, 681984, 684032, 686080, 688128, 690176, 692224, 694272, 696320, 698368, 700416, 702464, 704512, 706560, 708608, 710656, 712704, 714752, 716800, 718848, 720896, 722944, 724992, 727040, 729088, 731136, 733184, 735232, 737280, 739328, 741376, 743424, 745472, 747520, 749568, 751616, 753664, 755712, 757760, 759808, 761856, 763904, 765952, 768000, 770048, 772096, 774144, 776192, 778240, 780288, 782336, 784384, 786432, 788480, 790528, 792576, 794624, 796672, 798720, 800768, 802816, 804864, 806912, 808960, 811008, 813056, 815104, 817152, 819200, 821248, 823296, 825344, 827392, 829440, 831488, 833536, 835584, 837632, 839680, 841728, 843776, 845824, 847872, 849920, 851968, 854016, 856064, 858112, 860160, 862208, 864256, 866304, 868352, 870400, 872448, 874496, 876544, 878592, 880640, 882688, 884736, 886784, 888832, 890880, 892928, 894976, 897024, 899072, 901120, 903168, 905216, 907264, 909312, 911360, 913408, 915456, 917504, 919552, 921600, 923648, 925696, 927744, 929792, 931840, 933888, 935936, 937984, 940032, 942080, 944128, 946176, 948224, 950272, 952320, 954368, 956416, 958464, 960512, 962560, 964608, 966656, 968704, 970752, 972800, 974848, 976896, 978944, 980992, 983040, 985088, 987136, 989184, 991232, 993280, 995328, 997376, 999424, 1001472, 1003520, 1005568, 1007616, 1009664, 1011712, 1013760, 1015808, 1017856, 1019904, 1021952, 1024000, 1026048, 1028096, 1030144, 1032192, 1034240, 1036288, 1038336, 1040384, 1042432, 1044480, 1046528, 1048576]
        shared_ratios = [0.0,0.5,0.75,1.0]
        chunk_sizes = [64]
        n_heads, d_head = 32, 128
        n_requests = 32
        
        # æ‰“å¼€ä¸¤ä¸ªæ–‡ä»¶è®°å½•ç»“æœ
        with open('attention_latency_results_6.txt', 'w') as f, \
             open('add_seq_latency_4.txt', 'w') as add_seq_file:  
            
            # å†™å…¥è¡¨å¤´
            f.write("seq_len\tshared_ratio\tchunk_size\tlatency_us\n")
            add_seq_file.write("seq_len\tavg_latency_us\n")
            
            for seq_len in seq_lengths:
                for shared_ratio in shared_ratios:
                    n_shared = round(seq_len * shared_ratio)
                    print(f"\nTesting: seq_len={seq_len}, shared_ratio={shared_ratio}, n_shared={n_shared}")

                    # ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œç¡®ä¿æ‰€æœ‰å¼ é‡éƒ½åœ¨cuda:1ä¸Š
                    keys = [torch.randn((seq_len, n_heads, d_head), device=self.device) for _ in range(n_requests)]
                    shared_keys = torch.randn((n_shared, n_heads, d_head), device=self.device)
                    for key in keys:
                        key[:n_shared, :, :] = shared_keys
                    values = [torch.randn((seq_len, n_heads, d_head), device=self.device) for _ in range(n_requests)]
                    shared_values = torch.randn((n_shared, n_heads, d_head), device=self.device)
                    for value in values:
                        value[:n_shared, :, :] = shared_values
                    qs = [torch.randn((1, n_heads, d_head), device=self.device) for _ in range(n_requests)]
                    q = torch.cat(qs, dim=0)

                    for chunk_size in chunk_sizes:
                        print(f"Testing chunk_size: {chunk_size}")
                        
                        # åˆ›å»ºAttentionå®ä¾‹
                        attn = Attention(
                            n_heads=n_heads,
                            d_head=d_head,
                            chunk_size=chunk_size,
                            memory_mb=27000,
                            dtype=self.dtype,
                            device=self.device,
                        )

                        # è®°å½•add_seqçš„æ—¶é—´
                        add_seq_latencies = []
                        
                        # æ·»åŠ åºåˆ—å¹¶è®°å½•æ—¶é—´
                        for i in range(n_requests):
                            torch.cuda.synchronize()
                            start_time = time.time()
                            attn.add_seq(
                                tokens=list(range(n_shared)) + [random.randint(n_shared, seq_len) for _ in range(seq_len - n_shared)],
                                k=keys[i], v=values[i],
                            )
                            torch.cuda.synchronize()
                            
                            end_time = time.time()
                            latency = (end_time - start_time) * 1e6
                            # è·³è¿‡ç¬¬ä¸€æ¬¡çš„ç»“æœ
                            if i > 0:  # åªè®°å½•ç¬¬ä¸€æ¬¡ä¹‹åçš„ç»“æœ
                                add_seq_latencies.append(latency)
                            print(f"Add seq {i+1}/{n_requests}: {latency:.2f} Î¼s")
                        
                        # è®¡ç®—å¹¶ä¿å­˜add_seqçš„å¹³å‡å»¶è¿Ÿï¼ˆä¸åŒ…å«ç¬¬ä¸€æ¬¡ï¼‰
                        avg_add_seq_latency = sum(add_seq_latencies) / len(add_seq_latencies)
                        add_seq_file.write(f"{seq_len}\t{avg_add_seq_latency:.2f}\n")
                        add_seq_file.flush()
                        print(f"ğŸ“Š Average add_seq latency for seq_len={seq_len}: {avg_add_seq_latency:.2f} Î¼s (excluding first run)")
                        # é¢„çƒ­é˜¶æ®µ - 3æ¬¡
                        print(f"ğŸ”¥ Warming up for seq_len={seq_len} (3 rounds)")
                        for _ in range(3):
                            _ = attn.forward(q=q)
                            torch.cuda.synchronize()

                        # æµ‹è¯•é˜¶æ®µ - 10æ¬¡
                        latencies = []
                        print(f"ğŸš€ Testing seq_len={seq_len} (10 rounds)")
                        for i in range(10):
                            torch.cuda.synchronize()
                            start_time = time.time()
                            _ = attn.forward(q=q)
                            torch.cuda.synchronize()
                            end_time = time.time()
                            latency = (end_time - start_time) * 1e6
                            # è·³è¿‡ç¬¬ä¸€æ¬¡çš„ç»“æœ
                            if i > 0:  # åªè®°å½•ç¬¬ä¸€æ¬¡ä¹‹åçš„ç»“æœ
                                latencies.append(latency)
                            print(f"Run {i+1}/10: {latency:.2f} Î¼s")

                        # è®¡ç®—å¹³å‡å»¶è¿Ÿï¼ˆä¸åŒ…å«ç¬¬ä¸€æ¬¡ï¼‰
                        avg_latency = sum(latencies) / len(latencies)
                        print(f"ğŸ“Š Average latency: {avg_latency:.2f} Î¼s (excluding first run)")
                        
                        # è®°å½•ç»“æœ
                        f.write(f"{seq_len}\t{shared_ratio}\t{chunk_size}\t{avg_latency:.2f}\n")
                        f.flush()  # ç¡®ä¿ç«‹å³å†™å…¥æ–‡ä»¶

                        # æ¸…ç†å†…å­˜
                        del attn
                        torch.cuda.empty_cache()
                        gc.collect()
                        torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰GPUæ“ä½œå®Œæˆ

        # åœ¨æ–‡ä»¶å†™å…¥å®Œæˆåæ·»åŠ ç»˜å›¾ä»£ç 
        '''
        with open('attention_latency_results_3.txt', 'r') as f:
            # è·³è¿‡æ ‡é¢˜è¡Œ
            next(f)
            # è¯»å–æ•°æ®
            data = []
            for line in f:
                # ä½¿ç”¨å•ä¸ªåˆ¶è¡¨ç¬¦åˆ†å‰²
                parts = line.strip().split('\t')
                if len(parts) == 4:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®
                    data.append({
                        'seq_len': int(parts[0]),
                        'latency': float(parts[3])
                    })
        
        # æ•´ç†æ•°æ®ç”¨äºç»˜å›¾
        seq_lengths = [d['seq_len'] for d in data]
        latencies = [d['latency'] for d in data]
        
        # åˆ›å»ºå›¾è¡¨
        plt.figure(figsize=(12, 6))
        plt.plot(seq_lengths, latencies, 'bo-', linewidth=2, markersize=8)
        
        # è®¾ç½®å¯¹æ•°åˆ»åº¦ä½†æ˜¾ç¤ºåŸå§‹æ•°å€¼
        plt.xscale('log')  # ç§»é™¤base=2
        plt.yscale('log')
        
        # è®¾ç½®xè½´åˆ»åº¦
        x_ticks = seq_lengths  # ä½¿ç”¨æ‰€æœ‰åºåˆ—é•¿åº¦ä½œä¸ºåˆ»åº¦
        plt.xticks(x_ticks, x_ticks, rotation=45)
        
        # è®¾ç½®yè½´æ ¼å¼åŒ–ï¼Œæ˜¾ç¤ºåŸå§‹æ•°å€¼
        plt.gca().yaxis.set_major_formatter(plt.ScalarFormatter())
        plt.gca().xaxis.set_major_formatter(plt.ScalarFormatter())
        
        # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾
        plt.title('Chunk Attention Latency vs Sequence Length', fontsize=14, pad=20)
        plt.xlabel('Sequence Length', fontsize=12)
        plt.ylabel('Latency (Î¼s)', fontsize=12)
        
        # æ·»åŠ ç½‘æ ¼
        plt.grid(True, which="both", ls="-", alpha=0.2)
        
        # ä¼˜åŒ–å¸ƒå±€ï¼Œç¡®ä¿åˆ»åº¦æ ‡ç­¾å®Œå…¨æ˜¾ç¤º
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plt.savefig('chunk_attention_latency.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("ğŸ“Š Performance plot saved to chunk_attention_latency.png")
        '''

if __name__ == "__main__":
    unittest.main()
# è¿›è¡Œ PyTorch ç›¸å…³æ“ä½œ
torch.cuda.empty_cache()


# è¿›è¡Œ Python å†…å­˜å›æ”¶
gc.collect()
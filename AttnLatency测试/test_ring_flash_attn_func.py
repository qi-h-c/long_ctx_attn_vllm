import torch
import torch.distributed as dist
from flash_attn import flash_attn_qkvpacked_func
from ring_flash_attn import ring_flash_attn_qkvpacked_func
from utils import log, set_seed
import time
import ring_flash_attn
if __name__ == "__main__":
    dist.init_process_group("nccl")
    rank = dist.get_rank()
    set_seed(rank)
    world_size = dist.get_world_size()
    dtype = torch.bfloat16
    device = torch.device(f"cuda:{rank}")

    # 修改配置
    batch_size = 32
    seq_lengths = [1024,2048, 4096, 6144, 8192, 10240, 12288, 14336, 16384, 18432, 20480, 22528, 24576, 26624, 28672, 30720, 32768, 34816, 36864, 38912, 40960, 43008, 45056, 47104, 49152, 51200, 53248, 55296, 57344, 59392, 61440, 63488, 65536, 67584, 69632, 71680, 73728, 75776, 77824, 79872, 81920, 83968, 86016, 88064, 90112, 92160, 94208, 96256, 98304, 100352, 102400, 104448, 106496, 108544, 110592, 112640, 114688, 116736, 118784, 120832, 122880, 124928, 126976, 129024, 131072, 133120, 135168, 137216, 139264, 141312, 143360, 145408, 147456, 149504, 151552, 153600, 155648, 157696, 159744, 161792, 163840, 165888, 167936, 169984, 172032, 174080, 176128, 178176, 180224, 182272, 184320, 186368, 188416, 190464, 192512, 194560, 196608, 198656, 200704, 202752, 204800, 206848, 208896, 210944, 212992, 215040, 217088, 219136, 221184, 223232, 225280, 227328, 229376, 231424, 233472, 235520, 237568, 239616, 241664, 243712, 245760, 247808, 249856, 251904, 253952, 256000, 258048, 260096, 262144, 264192, 266240, 268288, 270336, 272384, 274432, 276480, 278528, 280576, 282624, 284672, 286720, 288768, 290816, 292864, 294912, 296960, 299008, 301056, 303104, 305152, 307200, 309248, 311296, 313344, 315392, 317440, 319488, 321536, 323584, 325632, 327680, 329728, 331776, 333824, 335872, 337920, 339968, 342016, 344064, 346112, 348160, 350208, 352256, 354304, 356352, 358400, 360448, 362496, 364544, 366592, 368640, 370688, 372736, 374784, 376832, 378880, 380928, 382976, 385024, 387072, 389120, 391168, 393216, 395264, 397312, 399360, 401408, 403456, 405504, 407552, 409600, 411648, 413696, 415744, 417792, 419840, 421888, 423936, 425984, 428032, 430080, 432128, 434176, 436224, 438272, 440320, 442368, 444416, 446464, 448512, 450560, 452608, 454656, 456704, 458752, 460800, 462848, 464896, 466944, 468992, 471040, 473088, 475136, 477184, 479232, 481280, 483328, 485376, 487424, 489472, 491520, 493568, 495616, 497664, 499712, 501760, 503808, 505856, 507904, 509952, 512000, 514048, 516096, 518144, 520192, 522240, 524288, 526336, 528384, 530432, 532480, 534528, 536576, 538624, 540672, 542720, 544768, 546816, 548864, 550912, 552960, 555008, 557056, 559104, 561152, 563200, 565248, 567296, 569344, 571392, 573440, 575488, 577536, 579584, 581632, 583680, 585728, 587776, 589824, 591872, 593920, 595968, 598016, 600064, 602112, 604160, 606208, 608256, 610304, 612352, 614400, 616448, 618496, 620544, 622592, 624640, 626688, 628736, 630784, 632832, 634880, 636928, 638976, 641024, 643072, 645120, 647168, 649216, 651264, 653312, 655360, 657408, 659456, 661504, 663552, 665600, 667648, 669696, 671744, 673792, 675840, 677888, 679936, 681984, 684032, 686080, 688128, 690176, 692224, 694272, 696320, 698368, 700416, 702464, 704512, 706560, 708608, 710656, 712704, 714752, 716800, 718848, 720896, 722944, 724992, 727040, 729088, 731136, 733184, 735232, 737280, 739328, 741376, 743424, 745472, 747520, 749568, 751616, 753664, 755712, 757760, 759808, 761856, 763904, 765952, 768000, 770048, 772096, 774144, 776192, 778240, 780288, 782336, 784384, 786432, 788480, 790528, 792576, 794624, 796672, 798720, 800768, 802816, 804864, 806912, 808960, 811008, 813056, 815104, 817152, 819200, 821248, 823296, 825344, 827392, 829440, 831488, 833536, 835584, 837632, 839680, 841728, 843776, 845824, 847872, 849920, 851968, 854016, 856064, 858112, 860160, 862208, 864256, 866304, 868352, 870400, 872448, 874496, 876544, 878592, 880640, 882688, 884736, 886784, 888832, 890880, 892928, 894976, 897024, 899072, 901120, 903168, 905216, 907264, 909312, 911360, 913408, 915456, 917504, 919552, 921600, 923648, 925696, 927744, 929792, 931840, 933888, 935936, 937984, 940032, 942080, 944128, 946176, 948224, 950272, 952320, 954368, 956416, 958464, 960512, 962560, 964608, 966656, 968704, 970752, 972800, 974848, 976896, 978944, 980992, 983040, 985088, 987136, 989184, 991232, 993280, 995328, 997376, 999424, 1001472, 1003520, 1005568, 1007616, 1009664, 1011712, 1013760, 1015808, 1017856, 1019904, 1021952, 1024000, 1026048, 1028096, 1030144, 1032192, 1034240, 1036288, 1038336, 1040384, 1042432, 1044480, 1046528, 1048576]
    nheads = 32
    d = 128
    dropout_p = 0
    causal = True
    deterministic = False

    for seqlen in seq_lengths:
        if seqlen % world_size != 0:
            continue
            
        assert d % 8 == 0

        qkv = torch.randn(
            batch_size, seqlen, 3, nheads, d, device=device, dtype=dtype, requires_grad=True
        )
        dist.broadcast(qkv, src=0)

        local_qkv = qkv.chunk(world_size, dim=1)[rank].detach().clone()
        local_qkv.requires_grad = True

        # 执行足够多次测试以覆盖预热和测试阶段
        for _ in range(100):  # 30次预热 + 10次测试 + 额外余量
            ring_out, ring_lse = ring_flash_attn_qkvpacked_func(
                local_qkv,
                dropout_p=dropout_p,
                causal=causal,
                window_size=(-1, -1),
                alibi_slopes=None,
                deterministic=deterministic,
                return_attn_probs=True,
            )
            torch.cuda.synchronize()

        # 清理内存
        torch.cuda.empty_cache()
'''
set -ex
cd /home/qhc/ring-flash-attention
torchrun --nproc_per_node 2 test/test_ring_flash_attn_func.py
'''